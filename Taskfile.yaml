version: '3'

vars:
  TOOL_DIR: "{{.USER_WORKING_DIR}}/bin"
  CERTS_DIR: "{{.USER_WORKING_DIR}}/certs"
  # renovate: datasource=go depName=sigs.k8s.io/controller-tools
  CONTROLLER_TOOLS_VERSION: v0.18.0
  # renovate: datasource=go depName=fybrik.io/crdoc
  CRDOC_VERSION: v0.6.4
  # renovate: datasource=go depName=github.com/kyverno/chainsaw
  CHAINSAW_VERSION: v0.2.12
  # Container image configuration
  MILO_IMAGE_NAME: "ghcr.io/datum-cloud/milo"
  MILO_IMAGE_TAG: "dev"
  TEST_INFRA_CLUSTER_NAME: "test-infra"
  # Test infra repository configuration - can be overridden with environment variable
  TEST_INFRA_REPO_REF: 'v0.4.0'

  # --- PERF TESTING (ClusterLoader2) -------------------------------------------
  PERF_TEST_ROOT: "test/performance/clusterloader2"
  PERF_REPORT_DIR: "reports"
  # Default kubeconfig for local kind (your dev flow)
  PERF_KUBECONFIG: ".milo/kubeconfig"
  CL2_BIN_DEFAULT: "./bin/clusterloader"

includes:
  # Must set TASK_X_REMOTE_TASKFILES=1 to use this feature.
  #
  # See: https://taskfile.dev/experiments/remote-taskfiles
  test-infra:
    taskfile: https://raw.githubusercontent.com/datum-cloud/test-infra/{{.TEST_INFRA_REPO_REF}}/Taskfile.yml
    checksum: 4995ce0e9bc4290b3a0175dee77af2570d715b435d5f3ff65f9cb6c491837bcb
    vars:
      REPO_REF: "{{.TEST_INFRA_REPO_REF}}"

tasks:
  default:
    desc: List all available tasks
    cmds:
      - task --list
    silent: true

  kubectl:
    desc: Run kubectl commands against the Milo API server
    silent: true
    cmds:
      - KUBECONFIG=.milo/kubeconfig kubectl {{.CLI_ARGS}}

  dev:build:
    desc: Build the Milo container image for development
    silent: true
    cmds:
      - |
        set -e
        echo "Building Milo container image: {{.MILO_IMAGE_NAME}}:{{.MILO_IMAGE_TAG}}"
        docker build -t "{{.MILO_IMAGE_NAME}}:{{.MILO_IMAGE_TAG}}" .
        echo "Successfully built {{.MILO_IMAGE_NAME}}:{{.MILO_IMAGE_TAG}}"
    sources:
      - "**/*.go"
      - "go.mod"
      - "go.sum"
      - "Dockerfile"
    generates:
      - ".task-build-timestamp"
    method: timestamp

  dev:load:
    desc: Load the Milo container image into the kind cluster
    silent: true
    cmds:
      - |
        set -e
        echo "Loading image {{.MILO_IMAGE_NAME}}:{{.MILO_IMAGE_TAG}} into kind cluster '{{.TEST_INFRA_CLUSTER_NAME}}'..."
        kind load docker-image "{{.MILO_IMAGE_NAME}}:{{.MILO_IMAGE_TAG}}" --name "{{.TEST_INFRA_CLUSTER_NAME}}"
        echo "Successfully loaded image into kind cluster"

  dev:deploy:
    desc: Deploy complete Milo control plane (etcd + API server + controller manager) to test-infra cluster
    silent: true
    deps:
      - generate:code
    cmds:
      - |
        set -e
        echo "üöÄ Deploying complete Milo control plane to test-infra cluster..."

        # Deploy all components including etcd, API server, controller manager, webhooks, RBAC, and networking
        # Everything is deployed in the milo-system namespace for simplified testing
        echo "üìã Deploying Milo control plane with etcd storage backend..."
        task test-infra:kubectl -- apply -k config/overlays/test-infra/

        # Wait for etcd Helm release to be complete (ensures deployment is fully reconciled)
        echo "‚è≥ Waiting for etcd Helm release to be ready..."
        task test-infra:kubectl -- wait --for=condition=Ready helmrelease/etcd -n milo-system --timeout=300s

        # Wait for etcd pod readiness (API server needs etcd to store data)
        echo "‚è≥ Waiting for etcd pod to be ready..."
        task test-infra:kubectl -- wait --for=condition=Ready pod -l app.kubernetes.io/component=etcd -n milo-system --timeout=180s

        # Wait for API server to be ready (required for CRD installation)
        # API server must be running before we can install custom resources
        echo "‚è≥ Waiting for API server to be ready..."
        task test-infra:kubectl -- wait --for=condition=Ready pod -l app.kubernetes.io/name=milo-apiserver -n milo-system --timeout=180s

        # Install CRDs into Milo API server (defines custom resource schemas)
        # Controller manager needs these CRDs to watch and reconcile custom resources
        echo "üìã Installing core control plane CRDs into Milo API server..."
        task kubectl -- apply -k config/crd/overlays/core-control-plane/
        task kubectl -- wait --for=condition=Established customresourcedefinitions --all

        # Step 6b: Install infrastructure control plane CRDs into Milo API server
        # This includes ProjectControlPlanes needed by the controller manager
        echo "üìã Installing infrastructure control plane CRDs into the infrastructure cluster..."
        task test-infra:kubectl -- apply -k config/crd/overlays/infra-control-plane/
        task test-infra:kubectl -- wait --for=condition=Established customresourcedefinitions projectcontrolplanes.infrastructure.miloapis.com

        # Step 7: Verify CRDs are properly installed (sanity check)
        echo "‚úÖ Verifying CRDs are installed..."
        CRD_COUNT=$(task kubectl -- get crd --no-headers | grep miloapis | wc -l || echo "0")
        echo "Installed $CRD_COUNT Milo CRDs in API server"

        # Step 7b: Create test users in Milo API server
        echo "üë§ Creating test users in Milo API server..."
        task kubectl -- apply -f config/overlays/test-infra/resources/test-users.yaml

        # Step 8: Wait for controller manager (now that CRDs exist for it to reconcile)
        # Controller manager can only start successfully after CRDs are available
        echo "‚è≥ Waiting for controller manager to be ready..."
        task test-infra:kubectl -- wait --for=condition=Ready pod -l app.kubernetes.io/name=milo-controller-manager -n milo-system --timeout=120s

        # Step 8b: Install webhook configurations to Milo API server
        # Webhooks validate and mutate resources submitted to the Milo API server
        echo "üîí Installing webhook configurations to Milo API server..."
        task kubectl -- apply -k config/webhook/
        # Webhook configurations don't have conditions to wait for - they're ready immediately after creation
        echo "‚úÖ Webhook configurations installed successfully"

        WEBHOOK_COUNT=$(task kubectl -- get mutatingwebhookconfigurations,validatingwebhookconfigurations --no-headers | grep resourcemanager.miloapis.com | wc -l || echo "0")
        echo "Installed $WEBHOOK_COUNT webhook configurations in Milo API server"

        # Update kubeconfig for easy developer access
        echo "üìù Updating kubeconfig for developer access..."

        echo ""
        echo "‚úÖ Milo API server and storage deployed successfully!"
        echo ""
        echo "üìä Status:"
        echo "  etcd: task test-infra:kubectl -- get pods -n milo-system -l app.kubernetes.io/component=etcd"
        echo "  API server: task test-infra:kubectl -- get pods -n milo-system -l app.kubernetes.io/name=milo-apiserver"
        echo "  Controller manager: task test-infra:kubectl -- get pods -n milo-system -l app.kubernetes.io/name=milo-controller-manager"
        echo ""
        echo "üîó Access:"
        echo "  Gateway: https://localhost:30443 (via Envoy Gateway)"
        echo ""
        echo "üîê Authentication:"
        echo "  Kubeconfig: .milo/kubeconfig"
        echo "  Usage: task kubectl -- <command>"
        echo ""
        echo "üéØ Test custom resources:"
        echo "  task kubectl -- get organizations"
        echo "  task kubectl -- get projects"
        echo "  task kubectl -- get users    # Should show 'admin' and 'test-user'"
        echo ""
        echo "üìã Available tokens:"
        echo "  Admin: test-admin-token (system:masters)"
        echo "  User: test-user-token (system:authenticated)"

  dev:setup:
    desc: Complete setup of test-infra cluster with full Milo control plane
    silent: true
    cmds:
      - task: test-infra:cluster-up
      - task: dev:build
      - task: dev:load
      - task: dev:deploy

  dev:redeploy:
    desc: Quick rebuild and redeploy for development iterations
    deps:
      - dev:load
    cmds:
      - |
        set -e
        echo "Redeploying Milo controller manager..."

        # Restart the deployment to pick up new image
        task test-infra:kubectl -- rollout restart deployment/milo-controller-manager -n milo-system

        # Wait for rollout to complete
        echo "Waiting for rollout to complete..."
        task test-infra:kubectl -- rollout status deployment/milo-controller-manager -n milo-system --timeout=120s

        echo "‚úÖ Redeployment complete!"
        echo "Check logs with: task test-infra:kubectl -- logs -n milo-system -l app.kubernetes.io/name=milo-controller-manager"

  dev:install-observability:
    desc: "Install test-infra observability, then apply Milo-specific scraping manifests"
    silent: true
    cmds:
      - |
        set -euo pipefail
        echo "üîß Installing base observability stack via test-infra‚Ä¶"
        task test-infra:install-observability

        echo "üìé Applying Milo observability (ServiceMonitor + cert)‚Ä¶"
        task test-infra:kubectl -- apply -k config/components/prometheus-monitoring/

        echo "‚úÖ Observability overlay applied."

  test:end-to-end:
    desc: Run end to end tests using Chainsaw against Milo API server. Pass directory names to run specific tests (e.g., 'task test:end-to-end -- quota' or 'task test:end-to-end -- quota group')
    deps:
      - task: install-go-tool
        vars:
          NAME: chainsaw
          PACKAGE: github.com/kyverno/chainsaw
          VERSION: "{{.CHAINSAW_VERSION}}"
    cmds:
      - |
        set -e
        echo "Running Chainsaw end to end tests against Milo API server..."
        echo "Using Milo API server via .milo/kubeconfig"

        # Verify Milo kubeconfig exists
        if [ ! -f ".milo/kubeconfig" ]; then
          echo "Error: Milo kubeconfig not found at .milo/kubeconfig"
          echo "Please run 'task dev:setup' to set up the test infrastructure first."
          exit 1
        fi

        # Verify connectivity to Milo API server
        echo "Verifying connectivity to Milo API server..."
        if ! KUBECONFIG=.milo/kubeconfig kubectl get --raw /healthz &>/dev/null; then
          echo "Error: Cannot connect to Milo API server"
          echo "Please ensure the test infrastructure is running with 'task dev:setup'"
          echo "You can check the status with:"
          echo "  task test-infra:kubectl -- get pods -n milo-system"
          exit 1
        fi
        echo "‚úì Successfully connected to Milo API server"

        # Determine test paths based on CLI arguments
        if [ -z "{{.CLI_ARGS}}" ]; then
          # No arguments provided - run all tests
          echo "No test directories specified - running all end-to-end tests..."
          TEST_PATHS="test/"
        else
          # Arguments provided - construct test paths
          echo "Running tests for specified directories: {{.CLI_ARGS}}"
          TEST_PATHS=""
          for dir in {{.CLI_ARGS}}; do
            if [ -d "test/$dir" ]; then
              TEST_PATHS="$TEST_PATHS test/$dir"
            else
              echo "Warning: Test directory 'test/$dir' does not exist, skipping..."
            fi
          done

          # Check if we found any valid test directories
          if [ -z "$TEST_PATHS" ]; then
            echo "Error: No valid test directories found for arguments: {{.CLI_ARGS}}"
            echo "Available test directories:"
            ls -1 test/ | grep -v "^config$" | grep -v "^docker-compose.yaml$" || true
            exit 1
          fi
        fi

        echo "Test paths: $TEST_PATHS"
        KUBECONFIG=.milo/kubeconfig "{{.TOOL_DIR}}/chainsaw" test $TEST_PATHS --selector "requires!=authorization-provider"
    silent: true

  test:unit:
    desc: Run unit tests
    cmds:
      - |
        set -e
        echo "Running unit tests..."
        go test ./... -v
    silent: true

  # Code generation tasks
  generate:
    desc: Generate code (alias for generate:code for backward compatibility)
    cmds:
      - task: generate:code
      - task: generate:docs

  generate:code:
    desc: Generate code including deepcopy, objects, CRDs, and potentially protobuf marshallers
    deps:
      - task: install-go-tool
        vars:
          NAME: controller-gen
          PACKAGE: sigs.k8s.io/controller-tools/cmd/controller-gen
          VERSION: "{{.CONTROLLER_TOOLS_VERSION}}"
    cmds:
      - echo "Generating deepcopy and object files..."
      - "\"{{.TOOL_DIR}}/controller-gen\" object paths=\"./pkg/apis/...\""
      - echo "Generating CRD manifests for each package..."
      - |
        set -e
        for package_dir in pkg/apis/*/; do
          package_name=$(basename "$package_dir")
          echo "Generating CRDs for package: $package_name"
          mkdir -p "config/crd/bases/$package_name"
          "{{.TOOL_DIR}}/controller-gen" crd paths="./$package_dir..." output:dir="./config/crd/bases/$package_name"
        done
      - echo "Generating webhook files..."
      - "\"{{.TOOL_DIR}}/controller-gen\" webhook paths=\"./internal/webhooks/...\" output:dir=\"./config/webhook\""
      # Generate RBAC rules for the controllers.
      - echo "Generating RBAC rules for the controllers..."
      - "\"{{.TOOL_DIR}}/controller-gen\" rbac:roleName=milo-controller-manager paths=\"./internal/controllers/...\" output:dir=\"./config/controller-manager/overlays/core-control-plane/rbac\""
    silent: true

  generate:docs:
    desc: Generate API docs
    deps:
      - task: install-go-tool
        vars:
          NAME: crdoc
          PACKAGE: fybrik.io/crdoc
          VERSION: "{{.CRDOC_VERSION}}"
    cmds:
      - |
        set -e ;
        mkdir -p docs/api ;
        for crdmanifest in config/crd/bases/*; do
          filename="$(basename -s .resourcemanager.miloapis.com.yaml $crdmanifest)" ;
          filename="${filename#apiextensions.k8s.io_v1_customresourcedefinition_}" ;
          bin/crdoc --resources $crdmanifest --output docs/api/$filename.md ;
        done;
    silent: true

  install-go-tool:
    desc: Install a Go tool to {{.TOOL_DIR}}/{{.NAME}} (symlinked from {{.TOOL_DIR}}/{{.NAME}}-{{.VERSION}})
    silent: true
    internal: true
    # vars: - Variables that need to be set when depending on this task
    #   NAME:
    #   PACKAGE:
    #   VERSION:
    cmds:
      - mkdir -p {{.TOOL_DIR}}
      - |
        set -e
        # Capture Taskfile vars into shell vars for clarity and safety in the script
        _NAME="{{.NAME}}"
        _PACKAGE="{{.PACKAGE}}"
        _VERSION="{{.VERSION}}"
        _TOOL_DIR="{{.TOOL_DIR}}"

        _VERSIONED_TOOL_PATH="$_TOOL_DIR/$_NAME-$_VERSION" # e.g., ./bin/crdoc-v0.6.4
        _SYMLINK_PATH="$_TOOL_DIR/$_NAME"                 # e.g., ./bin/crdoc (this is where go install puts it first)

        # Check if the correctly versioned binary already exists
        if [ ! -f "$_VERSIONED_TOOL_PATH" ]; then
          echo "Downloading $_PACKAGE@$_VERSION (binary name: $_NAME) to $_VERSIONED_TOOL_PATH"

          # Ensure the path where `go install` will place the binary (before mv) is clear.
          # This is $_SYMLINK_PATH (e.g., ./bin/crdoc).
          if [ -d "$_SYMLINK_PATH" ]; then
            echo "Error: Target path $_SYMLINK_PATH for 'go install' is an existing directory. Please remove it manually."
            exit 1
          fi
          # Remove if it's a file or symlink, to mimic `rm -f $(1)` from Makefile.
          # This ensures 'go install' doesn't conflict with an existing symlink or wrong file.
          echo "Preparing $_SYMLINK_PATH for new installation..."
          rm -f "$_SYMLINK_PATH" || true

          echo "Installing with GOBIN=$_TOOL_DIR..."
          # 'go install' will place the executable (named $_NAME) into $_TOOL_DIR.
          # This relies on $_NAME being the actual binary name derived from $_PACKAGE.
          if ! GOBIN="$_TOOL_DIR" go install "$_PACKAGE@$_VERSION"; then
            echo "Failed to 'go install $_PACKAGE@$_VERSION' with GOBIN=$_TOOL_DIR"
            exit 1
          fi

          # After `go install`, the binary should be at $_SYMLINK_PATH (e.g. $_TOOL_DIR/$_NAME)
          if [ ! -f "$_SYMLINK_PATH" ]; then
            echo "Error: 'go install' did not produce $_SYMLINK_PATH"
            # As a fallback, check if it was installed with the package basename if _NAME was different
            _PKG_BASENAME=$(basename "$_PACKAGE")
            if [ "$_PKG_BASENAME" != "$_NAME" ] && [ -f "$_TOOL_DIR/$_PKG_BASENAME" ]; then
                echo "Found $_TOOL_DIR/$_PKG_BASENAME instead (package basename). Moving this one."
                mv "$_TOOL_DIR/$_PKG_BASENAME" "$_VERSIONED_TOOL_PATH"
            else
                echo "Please ensure the NAME variable ('$_NAME') accurately matches the binary name produced by 'go install $_PACKAGE'."
                exit 1
            fi
          else
            # Binary $_SYMLINK_PATH was created as expected. Now move it to its versioned path.
            echo "Moving installed binary from $_SYMLINK_PATH to $_VERSIONED_TOOL_PATH"
            mv "$_SYMLINK_PATH" "$_VERSIONED_TOOL_PATH"
          fi

          # Create/update the symlink (e.g., ./bin/crdoc -> crdoc-v0.6.4)
          # The target of the symlink is relative to _TOOL_DIR.
          echo "Creating/updating symlink: $_SYMLINK_PATH -> $_NAME-$_VERSION (within $_TOOL_DIR)"
          (cd "$_TOOL_DIR" && ln -sf "$_NAME-$_VERSION" "$_NAME")
          echo "Tool $_NAME is now available at $_SYMLINK_PATH (points to $_VERSIONED_TOOL_PATH)"
        fi

  validate-kustomizations:
      desc: Validate all kustomization.yaml files using kustomize build
      cmds:
        - echo "# Kustomize Validation Results"
        - echo ""
        - |
          HAS_ERRORS=0
          KUSTOMIZATION_DIRS=$(find . -name "kustomization.yaml" -exec dirname {} \;)
          for dir in $KUSTOMIZATION_DIRS; do
            echo "üîç Validating: $dir"
            if ! OUTPUT=$(kustomize build "$dir" 2>&1); then
              echo ""
              echo "‚ùå Error in '$dir':"
              echo "----------------------------------------"
              echo "$OUTPUT"
              echo "----------------------------------------"
              echo ""
              HAS_ERRORS=1
            else
              echo "‚úÖ $dir is valid"
            fi
          done
          if [ "$HAS_ERRORS" -eq 1 ]; then
            echo ""
            echo "üö® One or more kustomizations failed validation."
            exit 1
          else
            echo ""
            echo "üéâ All kustomizations are valid."
          fi
      silent: false

  test-prometheus-rules:
      desc: Run unit tests for Prometheus alerting rules
      cmds:
        - echo "# Prometheus Rules Test Results"
        - echo ""
        - |
          HAS_ERRORS=0
          TEST_FILES=$(find test/prometheus-rules -name "*-tests.yaml" 2>/dev/null)
          
          if [ -z "$TEST_FILES" ]; then
            echo "‚ö†Ô∏è  No Prometheus test files found in test/prometheus-rules/"
            exit 0
          fi
          
          for test_file in $TEST_FILES; do
            test_dir=$(dirname "$test_file")
            test_name=$(basename "$test_file")
            echo "üîç Testing: $test_file"
            
            if ! OUTPUT=$(cd "$test_dir" && promtool test rules "$test_name" 2>&1); then
              echo ""
              echo "‚ùå Test failed for '$test_file':"
              echo "----------------------------------------"
              echo "$OUTPUT"
              echo "----------------------------------------"
              echo ""
              HAS_ERRORS=1
            else
              echo "‚úÖ $test_file passed"
            fi
          done
          
          if [ "$HAS_ERRORS" -eq 1 ]; then
            echo ""
            echo "üö® One or more Prometheus rule tests failed."
            exit 1
          else
            echo ""
            echo "üéâ All Prometheus rule tests passed."
          fi
      silent: false

# --- PERF TESTING (ClusterLoader2) -------------------------------------------
  perf:env:
    desc: Show effective perf test environment
    cmds:
      - |
        echo "CL2_BIN        = {{.CL2_BIN | default .CL2_BIN_DEFAULT}}"
        echo "PERF_TEST_ROOT = {{.PERF_TEST_ROOT}}"
        echo "PERF_REPORT_DIR= {{.PERF_REPORT_DIR}}"
        echo "KUBECONFIG     = {{.KUBECONFIG | default .PERF_KUBECONFIG}}"
        echo "ORG_NAME       = {{.ORG_NAME | default .CL2_OWNER_NAME | default "default-org"}}"
        echo "ORG_KUBECONFIG = {{.PERF_REPORT_DIR}}/kubeconfig.org.{{.ORG_NAME | default .CL2_OWNER_NAME | default "default-org"}}"
        echo "PARALLELISM    = {{.PARALLELISM | default "4"}}"
    silent: true

  perf:kube:org:
    desc: "Generate org-scoped kubeconfig (ephemeral; leaves base untouched)"
    env:
      BASE_KUBECONFIG: '{{.KUBECONFIG | default .PERF_KUBECONFIG}}'
      ORG_NAME:       '{{.ORG_NAME | default .CL2_OWNER_NAME | default "default-org"}}'
      APIGV:          '{{.APIGV | default "resourcemanager.miloapis.com/v1alpha1"}}'
      ORG_KUBECONFIG: '{{.PERF_REPORT_DIR}}/kubeconfig.org.{{.ORG_NAME | default .CL2_OWNER_NAME | default "default-org"}}'
    cmds:
      - |
        set -euo pipefail
        mkdir -p "{{.PERF_REPORT_DIR}}"

        # ensure defaults even if env not passed
        APIGV="${APIGV:-resourcemanager.miloapis.com/v1alpha1}"
        ORG_NAME="${ORG_NAME:-default-org}"

        BASE_SERVER="$(kubectl config view --kubeconfig "$BASE_KUBECONFIG" --raw --minify -o jsonpath='{.clusters[0].cluster.server}')"
        ORG_SERVER="${BASE_SERVER%/}/apis/${APIGV}/organizations/${ORG_NAME}/control-plane"

        USER_NAME="$(kubectl config view --kubeconfig "$BASE_KUBECONFIG" --raw --minify -o jsonpath='{.contexts[0].context.user}')"
        TOKEN="$(kubectl config view --kubeconfig "$BASE_KUBECONFIG" --raw --minify -o jsonpath='{.users[0].user.token}')"
        INSECURE="$(kubectl config view --kubeconfig "$BASE_KUBECONFIG" --raw --minify -o jsonpath='{.clusters[0].cluster.insecure-skip-tls-verify}')"
        if [ "${INSECURE:-}" = "true" ]; then INSECURE_FLAG="--insecure-skip-tls-verify=true"; else INSECURE_FLAG=""; fi

        if [ -n "${TOKEN:-}" ]; then
          kubectl config --kubeconfig "$ORG_KUBECONFIG" set-cluster "milo-org-${ORG_NAME}" --server="$ORG_SERVER" $INSECURE_FLAG
          kubectl config --kubeconfig "$ORG_KUBECONFIG" set-credentials "$USER_NAME" --token="$TOKEN"
          kubectl config --kubeconfig "$ORG_KUBECONFIG" set-context "milo-org-${ORG_NAME}" --cluster="milo-org-${ORG_NAME}" --user="$USER_NAME"
          kubectl config --kubeconfig "$ORG_KUBECONFIG" use-context "milo-org-${ORG_NAME}"
        else
          # fallback: copy minified config and patch only the server line
          tmp="$(mktemp)"
          kubectl config view --kubeconfig "$BASE_KUBECONFIG" --raw --minify > "$tmp"
          esc_old=$(printf '%s\n' "$BASE_SERVER" | sed -e 's/[\/&]/\\&/g')
          esc_new=$(printf '%s\n' "$ORG_SERVER" | sed -e 's/[\/&]/\\&/g')
          sed -E "s|(^[[:space:]]*server:[[:space:]]*)$esc_old|\1$esc_new|" "$tmp" > "$ORG_KUBECONFIG"
          rm -f "$tmp"
        fi

        # (optional) sanity check
        kubectl --kubeconfig "$ORG_KUBECONFIG" get --raw /apis >/dev/null

        echo "$ORG_KUBECONFIG" > "{{.PERF_REPORT_DIR}}/.org_kubeconfig_path"
        echo "ORG_KUBECONFIG=$ORG_KUBECONFIG"


  perf:org:bootstrap:
    desc: "Ensure Organization exists (root context)"
    env:
      KUBECONFIG: '{{.KUBECONFIG | default .PERF_KUBECONFIG}}'
      ORG_NAME:   '{{.ORG_NAME | default .CL2_OWNER_NAME | default "default-org"}}'
      ORG_TYPE:   '{{.ORG_TYPE | default "Standard"}}'
    cmds:
      - |
        set -euo pipefail
        cat <<'YAML' | ORG_NAME="${ORG_NAME}" ORG_TYPE="${ORG_TYPE}" envsubst | kubectl --kubeconfig "$KUBECONFIG" apply -f -
        apiVersion: resourcemanager.miloapis.com/v1alpha1
        kind: Organization
        metadata:
          name: ${ORG_NAME}
          labels: { cl2-role: owner }
        spec: { type: ${ORG_TYPE} }
        YAML

  perf:mgmt:create:
    desc: "CL2 mgmt: create Projects (org-context)"
    deps:
      - perf:org:bootstrap
      - perf:kube:org
    env:
      CL2_BIN: '{{.CL2_BIN | default .CL2_BIN_DEFAULT}}'
      ORG_KUBECONFIG: '{{.PERF_REPORT_DIR}}/kubeconfig.org.{{.ORG_NAME | default .CL2_OWNER_NAME | default "default-org"}}'
    cmds:
      - |
        set -euo pipefail
        tmp_over="$(mktemp)"
        cat "{{.PERF_TEST_ROOT}}/configs/overrides.yaml" > "$tmp_over"
        # Skip org creation + CRD checks when using org-context
        printf '\nCL2_ORG_REPLICAS: "0"\nCL2_CHECK_CRDS: "false"\n' >> "$tmp_over"
        "$CL2_BIN" \
          --testconfig="{{.PERF_TEST_ROOT}}/configs/00-projects.create.yaml" \
          --testoverrides="$tmp_over" \
          --provider=skeleton \
          --nodes=1 \
          --skip-cluster-verification=true \
          --delete-stale-namespaces=true \
          --enable-exec-service=false \
          --kubeconfig="$ORG_KUBECONFIG" \
          --report-dir="{{.PERF_REPORT_DIR}}/mgmt" \
          --v=4

  perf:mgmt:cleanup:
    desc: "CL2 mgmt: delete Projects (org-context) then delete Organization (root)"
    deps:
      - perf:kube:org
    env:
      CL2_BIN: '{{.CL2_BIN | default .CL2_BIN_DEFAULT}}'
      ORG_KUBECONFIG: '{{.PERF_REPORT_DIR}}/kubeconfig.org.{{.ORG_NAME | default .CL2_OWNER_NAME | default "default-org"}}'
      KUBECONFIG: '{{.KUBECONFIG | default .PERF_KUBECONFIG}}'
      ORG_NAME:   '{{.ORG_NAME | default .CL2_OWNER_NAME | default "default-org"}}'
    cmds:
      - |
        set -euo pipefail
        "$CL2_BIN" \
          --testconfig="{{.PERF_TEST_ROOT}}/configs/00-projects.cleanup.yaml" \
          --testoverrides="{{.PERF_TEST_ROOT}}/configs/overrides.yaml" \
          --provider=skeleton \
          --nodes=1 \
          --skip-cluster-verification=true \
          --delete-stale-namespaces=true \
          --enable-exec-service=false \
          --kubeconfig="$ORG_KUBECONFIG" \
          --report-dir="{{.PERF_REPORT_DIR}}/mgmt" \
          --v=4
        # Delete the org with the root kubeconfig (outside CL2)
        kubectl --kubeconfig "$KUBECONFIG" delete organizations.resourcemanager.miloapis.com "$ORG_NAME" --ignore-not-found

  perf:all:
    desc: "Create ‚Üí per-project CL2 tests ‚Üí cleanup"
    cmds:
      - task: perf:org:bootstrap
      - task: perf:kube:org
      - task: perf:mgmt:create
      - task: perf:projects
      - task: perf:mgmt:cleanup
